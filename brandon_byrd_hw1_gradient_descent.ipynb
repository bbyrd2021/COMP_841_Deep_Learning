{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Sp2yXCpgdQVY",
        "outputId": "b23c0f0f-6068-48ff-917e-cb2dc34e5814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/datateq/lib/python3.13/site-packages (2.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FfNOY_wMcDWl"
      },
      "outputs": [],
      "source": [
        "\n",
        "iteration = 0\n",
        "W_0 = 0\n",
        "W_1 = 0\n",
        "eta = 0.001 # step size\n",
        "conv_thresh = 0.01\n",
        "\n",
        "X = [1, 2, 4, 6, 8]\n",
        "Y = [2, 5, 6, 9, 11]\n",
        "\n",
        "weights = [W_0, W_1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vCKqwKHmiQEe"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# def get_gradients():\n",
        "#   pass\n",
        "\n",
        "def gradient_desc(a, b, eta, X, Y, conv_thresh, iteration):\n",
        "  converged = False\n",
        "  while not converged:\n",
        "\n",
        "    #get partial derivative of a and b\n",
        "    grad_a = -2 * sum((Y[i] - (a + b*X[i])) for i in range(len(X)))\n",
        "    grad_b = -2 * sum((Y[i] - (a + b * X[i])) * X[i] for i in range(len(X)))\n",
        "\n",
        "    #get magnitude of gradient\n",
        "    grad_mag = math.sqrt((grad_a)**2 + (grad_b)**2)\n",
        "\n",
        "    # check if we have convergence\n",
        "    if grad_mag < conv_thresh:\n",
        "      converged = True\n",
        "      print(\"=\"*60)\n",
        "      print(f\"Converged!\")\n",
        "      print(f\"iteration: {iteration} \\n   gradient magnitude: {grad_mag}\")\n",
        "\n",
        "    if iteration % 100 == 0:\n",
        "      print(\"=\"*60)\n",
        "      print(f\"iteration: {iteration} \\n   gradient magnitude: {grad_mag}\")\n",
        "\n",
        "    # update coefficients\n",
        "    a = a - eta * grad_a\n",
        "    b = b - eta * grad_b\n",
        "\n",
        "    # iterate count\n",
        "    iteration += 1\n",
        "\n",
        "  return a, b, iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tsVpO9yznROi",
        "outputId": "256b32e0-6b87-4a7d-9a3b-1c84d79c1064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "iteration: 0 \n",
            "   gradient magnitude: 362.0662922725616\n",
            "============================================================\n",
            "iteration: 100 \n",
            "   gradient magnitude: 2.676170860560416\n",
            "============================================================\n",
            "iteration: 200 \n",
            "   gradient magnitude: 2.0564409199245373\n",
            "============================================================\n",
            "iteration: 300 \n",
            "   gradient magnitude: 1.5802239384126953\n",
            "============================================================\n",
            "iteration: 400 \n",
            "   gradient magnitude: 1.2142861345241884\n",
            "============================================================\n",
            "iteration: 500 \n",
            "   gradient magnitude: 0.9330897859823554\n",
            "============================================================\n",
            "iteration: 600 \n",
            "   gradient magnitude: 0.7170110272615112\n",
            "============================================================\n",
            "iteration: 700 \n",
            "   gradient magnitude: 0.5509703577704025\n",
            "============================================================\n",
            "iteration: 800 \n",
            "   gradient magnitude: 0.4233802878891129\n",
            "============================================================\n",
            "iteration: 900 \n",
            "   gradient magnitude: 0.3253366821736797\n",
            "============================================================\n",
            "iteration: 1000 \n",
            "   gradient magnitude: 0.24999736595081742\n",
            "============================================================\n",
            "iteration: 1100 \n",
            "   gradient magnitude: 0.19210463008589795\n",
            "============================================================\n",
            "iteration: 1200 \n",
            "   gradient magnitude: 0.14761831093731925\n",
            "============================================================\n",
            "iteration: 1300 \n",
            "   gradient magnitude: 0.1134338392273204\n",
            "============================================================\n",
            "iteration: 1400 \n",
            "   gradient magnitude: 0.08716558128966297\n",
            "============================================================\n",
            "iteration: 1500 \n",
            "   gradient magnitude: 0.06698035271766711\n",
            "============================================================\n",
            "iteration: 1600 \n",
            "   gradient magnitude: 0.05146948582003237\n",
            "============================================================\n",
            "iteration: 1700 \n",
            "   gradient magnitude: 0.03955052284876008\n",
            "============================================================\n",
            "iteration: 1800 \n",
            "   gradient magnitude: 0.03039167445891953\n",
            "============================================================\n",
            "iteration: 1900 \n",
            "   gradient magnitude: 0.02335377157841935\n",
            "============================================================\n",
            "iteration: 2000 \n",
            "   gradient magnitude: 0.01794565967973375\n",
            "============================================================\n",
            "iteration: 2100 \n",
            "   gradient magnitude: 0.013789922551025787\n",
            "============================================================\n",
            "iteration: 2200 \n",
            "   gradient magnitude: 0.010596543529579961\n",
            "============================================================\n",
            "Converged!\n",
            "iteration: 2222 \n",
            "   gradient magnitude: 0.009999925267377408\n",
            "============================================================\n",
            "\n",
            "final a: 1.5511437620712774\n",
            "final b: 1.2018747340626836\n",
            "epochs: 2223\n",
            "\n"
          ]
        }
      ],
      "source": [
        "final_a, final_b, iterations = gradient_desc(W_0, W_1, eta, X, Y, conv_thresh, iteration)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"\"\"\n",
        "final a: {final_a}\n",
        "final b: {final_b}\n",
        "iterations: {iterations}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GQh1imBCnYPw"
      },
      "outputs": [],
      "source": [
        "def get_ground_truth(X, Y):\n",
        "  sum_x = sum(X)\n",
        "  sum_y = sum(Y)\n",
        "  sum_xy = sum(X[i] * Y[i] for i in range(len(X)))\n",
        "  sum_x2 = sum(X[i]**2 for i in range(len(X)))\n",
        "  n = len(X)\n",
        "\n",
        "  #solve grad for 0\n",
        "  b_truth = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n",
        "  a_truth = a = (sum_y - b_truth * sum_x) / n\n",
        "\n",
        "  return a_truth, b_truth\n",
        "\n",
        "def eval(model, X, Y):\n",
        "  a_truth, b_truth = get_ground_truth(X, Y)\n",
        "  a, b = model\n",
        "  RSS_gd = sum((Y[i] - (a + b*X[i]))**2 for i in range(len(X)))\n",
        "  RSS_truth = sum((Y[i] - (a_truth + b_truth*X[i]))**2 for i in range(len(X)))\n",
        "\n",
        "  diff_RSS = abs(RSS_gd - RSS_truth)\n",
        "\n",
        "  print(f\"RSS from Gradient Descent: {RSS_gd}\")\n",
        "  print(f\"RSS from Analytical: {RSS_truth}\")\n",
        "  print(f\"Difference: {diff_RSS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lJnSGRwJJfMO",
        "outputId": "aa4bfebd-0404-41f3-c364-ddabe2f02d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RSS from Gradient Descent: 1.8719701261724233\n",
            "RSS from Analytical: 1.871951219512195\n",
            "Difference: 1.8906660228301675e-05\n"
          ]
        }
      ],
      "source": [
        "eval((final_a, final_b), X, Y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "datateq",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
